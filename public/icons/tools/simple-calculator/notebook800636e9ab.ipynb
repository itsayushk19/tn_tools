{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\n\n# Define a custom dataset class\nclass ArticleDataset(Dataset):\n    def __init__(self, json_file, tokenizer, max_tag_length=10):\n        self.data = []\n        self.tokenizer = tokenizer\n        self.tag2idx = {}\n        self.max_tag_length = max_tag_length\n        \n        with open(json_file, 'r') as file:\n            articles = json.load(file)\n            for article in articles:\n                url = article['URL']\n                title = article['Title']\n                text = article['Text']\n                tags = article['Tags'].split(', ')\n                self.data.append((url, title, text, tags))\n                self.update_tags(tags)\n    \n    def update_tags(self, tags):\n        for tag in tags:\n            if tag not in self.tag2idx:\n                self.tag2idx[tag] = len(self.tag2idx)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        url, title, text, tags = self.data[index]\n        inputs = self.tokenizer.encode_plus(\n            title,\n            text,\n            add_special_tokens=True,\n            max_length=512,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        input_ids = inputs['input_ids'].squeeze()\n        attention_mask = inputs['attention_mask'].squeeze()\n        tag_indices = [self.tag2idx[tag] for tag in tags]\n        padded_tags = self.pad_tags(tag_indices)\n        tags = torch.tensor(padded_tags)\n        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'tags': tags}\n    \n    def pad_tags(self, tag_indices):\n        if len(tag_indices) >= self.max_tag_length:\n            return tag_indices[:self.max_tag_length]\n        else:\n            padded_tags = tag_indices + [0] * (self.max_tag_length - len(tag_indices))\n            return padded_tags\n\n# Load the labeled dataset\ndataset = ArticleDataset('/kaggle/input/filtered-wccftech-dataset-of-articles/filtered_data.json', tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n\n# Determine the number of tags\nnum_tags = len(dataset.tag2idx)\n\n# Define the training loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\nnum_epochs = 10\nbatch_size = 8\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tags = batch['tags'].to(device)\n        \n        # Compute the maximum length of non-zero elements in each batch\n        non_zero_lengths = torch.sum(tags != 0, dim=1)\n        max_length = torch.max(non_zero_lengths).item()\n        \n        # Adjust the target labels dynamically based on the maximum length\n        target_labels = tags[:, :max_length].contiguous().view(-1)\n        \n        outputs = model(input_ids, attention_mask=attention_mask, labels=target_labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    average_loss = total_loss / len(dataloader)\n    print(f'Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}')\n\n# Save the trained model\nmodel.save_pretrained('trained_model')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}