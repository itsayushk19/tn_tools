{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\n# Define your dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        \n        # Create a mapping of unique tags to integers\n        self.tag2idx = {tag: idx for idx, tag in enumerate(set([article[\"Tags\"] for article in self.data]))}\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        article = self.data[index]\n        text = article[\"Text\"]\n        label = article[\"Tags\"]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=128,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n        label = self.tag2idx[label]\n        \n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"label\": label\n        }\n\n# Load and preprocess your dataset from JSON file\ndef load_dataset_from_json(json_file):\n    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data\n\njson_file = \"/kaggle/input/filtered-wccftech-dataset-of-articles/filtered_data.json\"\ndataset = load_dataset_from_json(json_file)\ncustom_dataset = CustomDataset(dataset)\n\n# Determine the number of unique tags in your dataset\nnum_classes = len(set([article[\"Tags\"] for article in dataset]))\n\n# Split your dataset into training and validation sets\ntrain_size = int(0.8 * len(custom_dataset))\ntrain_dataset, val_dataset = torch.utils.data.random_split(custom_dataset, [train_size, len(custom_dataset) - train_size])\n\n# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8)\n\n# Define the BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n\n# Specify device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\n# Define training loop\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n            val_loss += loss.item()\n\n    val_loss /= len(val_loader)\n\n    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./\")\ntokenizer.save_pretrained(\"./\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:38:14.808778Z","iopub.execute_input":"2023-06-28T12:38:14.809142Z","iopub.status.idle":"2023-06-28T12:42:13.374018Z","shell.execute_reply.started":"2023-06-28T12:38:14.809114Z","shell.execute_reply":"2023-06-28T12:42:13.372926Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 512/512 [03:16<00:00,  2.61it/s]\nValidation: 100%|██████████| 128/128 [00:40<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 1.7497, Val Loss = 1.7292\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('./tokenizer_config.json',\n './special_tokens_map.json',\n './vocab.txt',\n './added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom newspaper import Article\n\n# Step 1: Load the saved model and tokenizer\nmodel_path = \"./\"  # Path where the saved model and tokenizer are stored\ntokenizer = BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path)\n\n# Step 2: Take input URL of the article\narticle_url = input(\"Enter the URL of the article: \")\n\n# Step 3: Fetch the article text using newspaper3k\narticle = Article(article_url)\narticle.download()\narticle.parse()\narticle_text = article.text\n\n# Step 4: Give the article text to the saved model to output predicted tags\ninputs = tokenizer.encode_plus(article_text, add_special_tokens=True, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\n# Pass the input through the model\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n# Get the predicted label index\npredicted_label_index = torch.argmax(outputs.logits, dim=1).item()\n\n# Step 5: Print the output\nprint(\"Predicted Tag: \", predicted_label_index)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T12:54:15.137103Z","iopub.execute_input":"2023-06-28T12:54:15.137575Z","iopub.status.idle":"2023-06-28T12:54:18.643105Z","shell.execute_reply.started":"2023-06-28T12:54:15.137539Z","shell.execute_reply":"2023-06-28T12:54:18.641996Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the URL of the article:  https://wccftech.com/pixel-7a-refresh-rate-reduces-90hz-to-60hz-under-direct-sunlight/?dark=1\n"},{"name":"stdout","text":"Predicted Tag:  85\n","output_type":"stream"}]}]}