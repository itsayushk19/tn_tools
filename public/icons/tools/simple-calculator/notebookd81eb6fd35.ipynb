{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.model_selection import train_test_split\n\n# Load the JSON file\nwith open('/kaggle/input/filtered-wccftech-dataset-of-articles/filtered_data.json', 'r') as f:\n    data = json.load(f)\n\n# Extract the data from the JSON file\nurls = []\ntitles = []\ntexts = []\ntags = []\nfor article in data:\n    urls.append(article['URL'])\n    titles.append(article['Title'])\n    texts.append(article['Text'])\n    tags.append(article['Tags'])\n\n# Create a DataFrame from the extracted data\ndf = pd.DataFrame({'URL': urls, 'Title': titles, 'Text': texts, 'Tags': tags})\n\n# Split the data into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the text data\ndef tokenize_text(text):\n    tokens = tokenizer.encode_plus(\n        text,\n        max_length=512,\n        truncation=True,\n        padding='max_length',\n        add_special_tokens=True,\n        return_attention_mask=True,\n        return_token_type_ids=False,\n        return_tensors='tf'\n    )\n    return tokens\n\ntrain_tokens = tokenize_text(train_df['Text'])\ntest_tokens = tokenize_text(test_df['Text'])\n\n# Create input arrays\ntrain_input = [train_tokens['input_ids'], train_tokens['attention_mask']]\ntest_input = [test_tokens['input_ids'], test_tokens['attention_mask']]\n\n# Encode the tags\ntags = df['Tags'].str.split(', ')\nunique_tags = set(tag for sublist in tags for tag in sublist)\ntag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n\ntrain_tags = [to_categorical([tag2idx[tag] for tag in sample_tags], num_classes=len(unique_tags)) for sample_tags in train_df['Tags'].str.split(', ')]\ntest_tags = [to_categorical([tag2idx[tag] for tag in sample_tags], num_classes=len(unique_tags)) for sample_tags in test_df['Tags'].str.split(', ')]\n\n# Convert the input arrays to TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_tags))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_tags))\n\n# Define the BERT model\nbert_model = TFBertModel.from_pretrained('bert-base-uncased', num_labels=len(unique_tags))\ninput_ids = tf.keras.Input(shape=(512,), dtype=tf.int32)\nattention_mask = tf.keras.Input(shape=(512,), dtype=tf.int32)\noutput = bert_model([input_ids, attention_mask])[0]\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n# Compile and train the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.fit(train_dataset.batch(16), epochs=5)\n\n# Evaluate the model\nloss, accuracy = model.evaluate(test_dataset.batch(16))\nprint(f\"Test loss: {loss:.4f}\")\nprint(f\"Test accuracy: {accuracy:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}